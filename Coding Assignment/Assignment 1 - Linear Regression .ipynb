{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part 1 -- DataGeneration function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to create the dataset to work on . \n",
    "## std_dev - Stand. Deviation , n - Size of xi and m - size of Beta  \n",
    "def generate_dataset(m, n, std_dev):\n",
    "    # We calculate the number of predictors, and create a beta_matrix\n",
    "    # With `m+1`(beta_matx) rows and 1 column, for matrix multiplication\n",
    "    beta_matx = np.random.rand(m+1,1)\n",
    "    # Similar as before, but with `n` rows and `beta_matrix` columns this time\n",
    "    x = np.random.random_sample((n,m))\n",
    "    x = np.hstack((np.matrix(np.ones(x.shape[0])).T, x))\n",
    "    e = np.random.normal(0, std_dev, (n,1))\n",
    "    # Since x is a n*beta_matrix, and coefficients is a beta*1 matrix\n",
    "    # we can use matrix multiplication to get the value of y for each\n",
    "    # set of values x1, x2 .. xp\n",
    "    # We need to transpose it to get a 1*n array from a n*1 matrix to use in the regression model\n",
    "    #print(beta_matx.shape,x.shape)\n",
    "    y = np.matmul(x, beta_matx)  + e\n",
    "      \n",
    "    return beta_matx,x, y,e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part 2 -- Gradient Dec. function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using it to calculate cost \n",
    "def MSE_cost(Y_hat,Y,n):\n",
    "    cost = 1/(2*n) * np.dot((Y-Y_hat).T, (Y-Y_hat))\n",
    "    return cost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad(X,Y_hat,Y,n):\n",
    "    grad = 1/n * np.dot(X.T,(Y_hat - Y))\n",
    "    return grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating multivarient linear_regression\n",
    "def LR(X,Y,epochs,threshold,lr):\n",
    "    cost_list = []\n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    beta = np.ones((m,1), dtype = int) \n",
    "    prev_cost = float('inf')\n",
    "    for i in range(epochs):\n",
    "        Y_hat = np.dot(X,beta)\n",
    "        #print(beta.shape)\n",
    "        cost = MSE_cost(Y_hat,Y,n)\n",
    "        cost_list.append(cost)\n",
    "        # if it reaches the threshold it will come out of the loop...\n",
    "        if(prev_cost - cost <= threshold):\n",
    "            break  \n",
    "        prev_cost = cost\n",
    "        grad = MSE_grad(X,Y_hat,Y,n)\n",
    "        beta = beta - lr * grad \n",
    "    return np.squeeze(cost), beta,cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have an array of coefficients, instead of a single one\n",
    "m = 2  # size of beta \n",
    "n = 100 # size for xi\n",
    "std_dev = .1\n",
    "Original_beta,X, y,e = generate_dataset(m, n, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) (100, 3) (100, 1) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the input data \n",
    "print(Original_beta.shape,X.shape,y.shape,e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_beta = [[0.17984201]\n",
      " [0.54165945]\n",
      " [0.12460277]], X = [[1.         0.84727365 0.70371933]\n",
      " [1.         0.92980007 0.8496215 ]\n",
      " [1.         0.57715976 0.24120899]\n",
      " [1.         0.17433266 0.78310061]\n",
      " [1.         0.18885338 0.55385844]\n",
      " [1.         0.44627179 0.24209172]\n",
      " [1.         0.96665739 0.28612196]\n",
      " [1.         0.06822171 0.67981514]\n",
      " [1.         0.51442318 0.44282098]\n",
      " [1.         0.2260873  0.99106646]\n",
      " [1.         0.9684999  0.22504111]\n",
      " [1.         0.07490763 0.34747127]\n",
      " [1.         0.79825753 0.21086097]\n",
      " [1.         0.50064334 0.47294256]\n",
      " [1.         0.61290018 0.3271913 ]\n",
      " [1.         0.57844012 0.32629558]\n",
      " [1.         0.79842513 0.7653811 ]\n",
      " [1.         0.36422214 0.01970919]\n",
      " [1.         0.35953773 0.81006502]\n",
      " [1.         0.99989533 0.91306543]\n",
      " [1.         0.15091937 0.2354647 ]\n",
      " [1.         0.36085548 0.96707901]\n",
      " [1.         0.85425807 0.1372076 ]\n",
      " [1.         0.9907277  0.49013619]\n",
      " [1.         0.60048746 0.36260567]\n",
      " [1.         0.17416312 0.10002682]\n",
      " [1.         0.42552775 0.00579807]\n",
      " [1.         0.45848286 0.25334373]\n",
      " [1.         0.71228487 0.39095119]\n",
      " [1.         0.84812852 0.83538462]\n",
      " [1.         0.94374096 0.51231996]\n",
      " [1.         0.08706786 0.98166542]\n",
      " [1.         0.90064775 0.07979421]\n",
      " [1.         0.26032841 0.65079029]\n",
      " [1.         0.08898214 0.59914903]\n",
      " [1.         0.45976715 0.80952237]\n",
      " [1.         0.02527732 0.76856281]\n",
      " [1.         0.3446353  0.2711209 ]\n",
      " [1.         0.67318433 0.62815199]\n",
      " [1.         0.02308233 0.03114003]\n",
      " [1.         0.38987653 0.91714839]\n",
      " [1.         0.47154495 0.07260354]\n",
      " [1.         0.70752651 0.42746941]\n",
      " [1.         0.15327011 0.9073726 ]\n",
      " [1.         0.47533647 0.13133878]\n",
      " [1.         0.31045387 0.86207159]\n",
      " [1.         0.25256977 0.31538035]\n",
      " [1.         0.36758208 0.08617312]\n",
      " [1.         0.93937327 0.3272624 ]\n",
      " [1.         0.74880751 0.47634211]\n",
      " [1.         0.71751909 0.2682496 ]\n",
      " [1.         0.63403932 0.58301534]\n",
      " [1.         0.84424663 0.38237808]\n",
      " [1.         0.31032163 0.23086043]\n",
      " [1.         0.82688481 0.04908146]\n",
      " [1.         0.97143613 0.00719448]\n",
      " [1.         0.07315869 0.74080527]\n",
      " [1.         0.16456294 0.02867617]\n",
      " [1.         0.91293825 0.1532959 ]\n",
      " [1.         0.69193909 0.52143574]\n",
      " [1.         0.55427185 0.74813163]\n",
      " [1.         0.96210617 0.5464302 ]\n",
      " [1.         0.49638362 0.44878794]\n",
      " [1.         0.52975336 0.9087297 ]\n",
      " [1.         0.19137989 0.82868206]\n",
      " [1.         0.66174226 0.52017707]\n",
      " [1.         0.97470296 0.01117096]\n",
      " [1.         0.13407962 0.94505805]\n",
      " [1.         0.85398811 0.09874881]\n",
      " [1.         0.28531525 0.12751714]\n",
      " [1.         0.59942325 0.10780016]\n",
      " [1.         0.88287004 0.27147317]\n",
      " [1.         0.24379307 0.35346128]\n",
      " [1.         0.4127986  0.22825583]\n",
      " [1.         0.40142252 0.90951416]\n",
      " [1.         0.04734693 0.82267819]\n",
      " [1.         0.33483219 0.04768317]\n",
      " [1.         0.49986625 0.15316418]\n",
      " [1.         0.23386269 0.67812634]\n",
      " [1.         0.54974572 0.72862233]\n",
      " [1.         0.22408897 0.68664158]\n",
      " [1.         0.30010175 0.04586942]\n",
      " [1.         0.7369371  0.93599541]\n",
      " [1.         0.58831685 0.75548148]\n",
      " [1.         0.52478705 0.48135185]\n",
      " [1.         0.08715251 0.16364418]\n",
      " [1.         0.10692814 0.4959013 ]\n",
      " [1.         0.65971452 0.93181768]\n",
      " [1.         0.65752032 0.63753789]\n",
      " [1.         0.95398007 0.97864734]\n",
      " [1.         0.51900146 0.71489297]\n",
      " [1.         0.57061938 0.18485517]\n",
      " [1.         0.10495576 0.63580458]\n",
      " [1.         0.75202157 0.74052972]\n",
      " [1.         0.16645678 0.05474113]\n",
      " [1.         0.06634775 0.44161149]\n",
      " [1.         0.50747112 0.37422462]\n",
      " [1.         0.89785407 0.11366114]\n",
      " [1.         0.81616399 0.72306512]\n",
      " [1.         0.57711447 0.22962296]], y = [[0.66600949]\n",
      " [0.80639729]\n",
      " [0.49414363]\n",
      " [0.21137253]\n",
      " [0.39757428]\n",
      " [0.23826591]\n",
      " [0.74727023]\n",
      " [0.3474307 ]\n",
      " [0.4195536 ]\n",
      " [0.44914458]\n",
      " [0.74593808]\n",
      " [0.15397816]\n",
      " [0.56173505]\n",
      " [0.50514206]\n",
      " [0.47704918]\n",
      " [0.60130822]\n",
      " [0.5192658 ]\n",
      " [0.3506069 ]\n",
      " [0.42016434]\n",
      " [0.96310312]\n",
      " [0.2804223 ]\n",
      " [0.50975245]\n",
      " [0.35899143]\n",
      " [0.83613981]\n",
      " [0.59054672]\n",
      " [0.32121274]\n",
      " [0.31395267]\n",
      " [0.52881015]\n",
      " [0.62774769]\n",
      " [0.78479669]\n",
      " [0.82296644]\n",
      " [0.34412928]\n",
      " [0.69470569]\n",
      " [0.52163575]\n",
      " [0.32000382]\n",
      " [0.37392481]\n",
      " [0.30085676]\n",
      " [0.52476098]\n",
      " [0.61049814]\n",
      " [0.24362551]\n",
      " [0.53790499]\n",
      " [0.45327593]\n",
      " [0.48396745]\n",
      " [0.34701849]\n",
      " [0.27084539]\n",
      " [0.34680505]\n",
      " [0.30018559]\n",
      " [0.4830327 ]\n",
      " [0.85643951]\n",
      " [0.70677747]\n",
      " [0.6052782 ]\n",
      " [0.71655975]\n",
      " [0.72971901]\n",
      " [0.48259934]\n",
      " [0.63426493]\n",
      " [0.65334307]\n",
      " [0.36871929]\n",
      " [0.30965137]\n",
      " [0.79589496]\n",
      " [0.59279428]\n",
      " [0.71235203]\n",
      " [1.03218927]\n",
      " [0.5207491 ]\n",
      " [0.58706904]\n",
      " [0.31501282]\n",
      " [0.47178057]\n",
      " [0.80858146]\n",
      " [0.54678272]\n",
      " [0.63439181]\n",
      " [0.11682044]\n",
      " [0.61253509]\n",
      " [0.71670822]\n",
      " [0.38931806]\n",
      " [0.44691042]\n",
      " [0.50213363]\n",
      " [0.34447886]\n",
      " [0.27277713]\n",
      " [0.58475878]\n",
      " [0.3737483 ]\n",
      " [0.52590854]\n",
      " [0.3316768 ]\n",
      " [0.48967835]\n",
      " [0.70335028]\n",
      " [0.60281882]\n",
      " [0.56457751]\n",
      " [0.29254101]\n",
      " [0.04347656]\n",
      " [0.50176948]\n",
      " [0.85853193]\n",
      " [0.78835475]\n",
      " [0.67131208]\n",
      " [0.47072487]\n",
      " [0.32929104]\n",
      " [0.72302787]\n",
      " [0.39131615]\n",
      " [0.14144579]\n",
      " [0.64974951]\n",
      " [0.49889981]\n",
      " [0.55277239]\n",
      " [0.69840501]], e = [[-0.06045168]\n",
      " [ 0.01705509]\n",
      " [-0.02837772]\n",
      " [-0.16047492]\n",
      " [ 0.04642575]\n",
      " [-0.21346873]\n",
      " [ 0.00817752]\n",
      " [ 0.04592891]\n",
      " [-0.0941073 ]\n",
      " [ 0.02335063]\n",
      " [ 0.0134582 ]\n",
      " [-0.10973416]\n",
      " [-0.07676456]\n",
      " [-0.00480811]\n",
      " [-0.07554494]\n",
      " [ 0.06749131]\n",
      " [-0.18841933]\n",
      " [-0.02897529]\n",
      " [-0.05536103]\n",
      " [ 0.12788787]\n",
      " [-0.01050616]\n",
      " [ 0.01394893]\n",
      " [-0.30066398]\n",
      " [ 0.05858845]\n",
      " [ 0.04026333]\n",
      " [ 0.03457001]\n",
      " [-0.09710293]\n",
      " [ 0.06905924]\n",
      " [ 0.01337624]\n",
      " [ 0.04146662]\n",
      " [ 0.06810173]\n",
      " [-0.0051921 ]\n",
      " [ 0.01707674]\n",
      " [ 0.11969413]\n",
      " [ 0.01730817]\n",
      " [-0.15582316]\n",
      " [ 0.01155799]\n",
      " [ 0.12446159]\n",
      " [-0.01225   ]\n",
      " [ 0.04740061]\n",
      " [ 0.03260344]\n",
      " [ 0.00897055]\n",
      " [-0.13237685]\n",
      " [-0.02890487]\n",
      " [-0.18283229]\n",
      " [-0.10861374]\n",
      " [-0.05576049]\n",
      " [ 0.09334897]\n",
      " [ 0.12699928]\n",
      " [ 0.06198325]\n",
      " [ 0.00336055]\n",
      " [ 0.12063902]\n",
      " [ 0.04493747]\n",
      " [ 0.10590284]\n",
      " [ 0.00041727]\n",
      " [-0.05358296]\n",
      " [ 0.0569438 ]\n",
      " [ 0.03709916]\n",
      " [ 0.10245023]\n",
      " [-0.02681542]\n",
      " [ 0.13906415]\n",
      " [ 0.26312665]\n",
      " [ 0.01611599]\n",
      " [ 0.00705088]\n",
      " [-0.071748  ]\n",
      " [-0.13131589]\n",
      " [ 0.09939045]\n",
      " [ 0.17655836]\n",
      " [-0.0203253 ]\n",
      " [-0.23345426]\n",
      " [ 0.09457761]\n",
      " [ 0.024825  ]\n",
      " [ 0.03338097]\n",
      " [ 0.01503084]\n",
      " [-0.00847067]\n",
      " [ 0.03648295]\n",
      " [-0.09437135]\n",
      " [ 0.11507481]\n",
      " [-0.01726407]\n",
      " [-0.0424968 ]\n",
      " [-0.05510256]\n",
      " [ 0.14156794]\n",
      " [ 0.00771171]\n",
      " [ 0.01017434]\n",
      " [ 0.04050186]\n",
      " [ 0.04510151]\n",
      " [-0.25607476]\n",
      " [-0.15152019]\n",
      " [ 0.24309883]\n",
      " [-0.03016175]\n",
      " [ 0.12127038]\n",
      " [-0.04123198]\n",
      " [ 0.01337574]\n",
      " [ 0.04357422]\n",
      " [ 0.11449035]\n",
      " [-0.12936012]\n",
      " [ 0.14840155]\n",
      " [-0.18143583]\n",
      " [-0.15924848]\n",
      " [ 0.17735184]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original_beta = {}, X = {}, y = {}, e = {}\".format(Original_beta,X,y,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the grad_fun - Test 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "threshold = 0.00001\n",
    "lr = 0.001 \n",
    "cost,beta,cost_list = LR(X,y,epochs,threshold,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost - [[0.01373251]], \n",
      " beta - [[0.08866592]\n",
      " [0.52907867]\n",
      " [0.48463565]] \n",
      " Original_beta - [[0.17984201]\n",
      " [0.54165945]\n",
      " [0.12460277]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost - {}, \\n beta - {} \\n Original_beta - {}\".format(cost,beta,Original_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion -- \n",
    "\n",
    "## If the size of data is less then < 100  then the cost is large and prediction is not at all accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - 2 \n",
    "\n",
    "## changing the Input value .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = 3  # size of beta \n",
    "n = 150 # size for xi\n",
    "std_dev = .1\n",
    "Original_beta,X, y,e = generate_dataset(m, n, std_dev)\n",
    "\n",
    "epochs = 10000\n",
    "threshold = 0.00001\n",
    "lr = 0.0001 \n",
    "cost,beta,cost_list = LR(X,y,epochs,threshold,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost - [[0.03601662]], \n",
      " beta - [[0.47652296]\n",
      " [0.71925274]\n",
      " [0.73750976]\n",
      " [0.74033006]] \n",
      " Original_beta - [[0.17173212]\n",
      " [0.66258196]\n",
      " [0.80781447]\n",
      " [0.84051408]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost - {}, \\n beta - {} \\n Original_beta - {}\".format(cost,beta,Original_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently we can't figure out using this data as it is not constant. However while increasing the number of parameters the cost increased and kept on increasing. Moreover while reducing its parameters we don't get proper results as it cannot establish any relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
